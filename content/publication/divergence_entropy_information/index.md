---
abstract : "Information theory is a mathematical theory of learning with deep connections with topics as diverse as artificial intelligence, statistical physics, and biological evolution. Many primers on the topic paint a broad picture with relatively little mathematical sophistication, while many others develop specific application areas in detail. In contrast, these informal notes aim to outline some elements of the information-theoretic way of thinking, by cutting a rapid and interesting path through some of the theory's foundational concepts and theorems. They are aimed at practicing systems scientists who are interested in exploring potential connections between information theory and their own fields. The main mathematical prerequisite for the notes is comfort with elementary probability, including sample spaces, conditioning, and expectations.  We take the Kullback-Leibler divergence as our foundational concept, and then proceed to develop the entropy and mutual information. We discuss some of the main foundational results, including the Chernoff bounds as a characterization of the divergence; Gibbs' Theorem; and the Data Processing Inequality. A recurring theme is that the definitions of information theory support natural theorems that sound obvious when translated into English. More pithily, information theory makes common sense precise. Since the focus of the notes is not primarily on technical details, proofs are provided only where the relevant techniques are illustrative of broader themes. Otherwise, proofs and intriguing tangents are referenced in liberally-sprinkled footnotes. The notes close with a highly nonexhaustive list of references to resources and other perspectives on the field."
abstract_short : "A nonexhaustive introduction to information theory that emphasizes the mathematics of learning, and its connections to statistics and physics."
authors : ["Phil Chodrow"]
date : "2017-08-24"
image_preview : ""
math : true
publication_types : ["3"]
publication : "arXiv: 1708.07459"
publication_short : "arXiv: 1708.07459"
selected : false
title : "Divergence, entropy, information: an opinionated introduction to information theory"
url_code : ""
url_dataset : ""
url_pdf : "https://arxiv.org/abs/1708.07459"
url_project : ""
url_slides : ""
url_video : ""
categories: ["featured"]
summary: "An invitation to information theory for complex systems scientists and other nonspecialists."


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

---


