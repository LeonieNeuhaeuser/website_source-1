---
title: "Information Measures Segregation"
author: "Phil"
summary: "An introduction to the mutual information and its use in studying segregation. (Information Theory and Segregation, Part 2)"
tags: [r, information theory, research]
date: 2017-06-19
---



<p><a href="/post/2017-03-06-info_segregation_1/">Last time</a>, we studied the information-theoretic idea of <em>entropy</em> and its relationship to our intuitive concept of <em>diversity</em>. We saw that the entropy <span class="math inline">\(H\)</span> is highest precisely when all possible groups are represented equally in a population, and lowest when only one group is represented. High entropy, high diversity.</p>
<p>Now let’s move on to an information-theoretic view of <em>segregation</em>. It’s important to remember that segregation is a fundamentally distinct concept from diversity. We can remind ourselves of this difference by recalling the example of the tea drinkers in the cafe. There are drinkers who prefer <strong><font color="#543005">black</font></strong>, <strong><font color="#D73027">red</font></strong>, and <strong><font color="#A1D76A">green</font></strong> teas; on three different days, the population of the cafe looked like this:</p>
<p><img src="/post/2017-06-19-info-segregation-2_files/figure-html/unnamed-chunk-2-1.png" width="600px" height="300px" style="display: block; margin: auto;" /></p>
<p>We’re interested in whether or not the shop is <em>diverse</em>, and whether or not it is <em>segregated</em> (by room). Intuitively,</p>
<ul>
<li><strong>Friday</strong> is neither diverse nor segregated.</li>
<li><strong>Saturday</strong> is diverse, but fairly unsegregated.</li>
<li><strong>Sunday</strong> is diverse but highly segregated.</li>
</ul>
<p>As you may remember, the entropy <span class="math inline">\(H\)</span> could distinguish Friday (<span class="math inline">\(H = 0\)</span>) from Saturday and Sunday (<span class="math inline">\(H = 1.10\)</span>), but couldn’t distinguish Saturday and Sunday from each other. For that we need more subtle ideas. Let’s dive in!</p>
<div id="local-entropy" class="section level1">
<h1>Local Entropy</h1>
<p>One way to think about segregation is just to think about diversity again – but on a lower level of organization. To see this, compare Saturday and Sunday again, <em>within each room</em>. Computing entropies for each of the three rooms individually, on each day, we have:</p>
<table>
<thead>
<tr class="header">
<th align="left">Day</th>
<th align="right">Garden</th>
<th align="right">Left</th>
<th align="right">Right</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Friday</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.0</td>
</tr>
<tr class="even">
<td align="left">Saturday</td>
<td align="right">1.08</td>
<td align="right">1.08</td>
<td align="right">1.1</td>
</tr>
<tr class="odd">
<td align="left">Sunday</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.0</td>
</tr>
</tbody>
</table>
<p>Just as we’d expect, the entropies in the individual rooms are zero on Friday and Sunday, but high on Saturday, reflecting diversity in each room.</p>
<p>Mathematically, the entropy of each individual room is an example of <em>conditioned</em> or <em>local</em> entropy. To write this down, let’s allow <span class="math inline">\(Y\)</span> to be a random variable denoting the tea preference of a randomly selected customer, and <span class="math inline">\(X\)</span> be a random variable denoting where they sit in the cafe. Then, we can write the overall entropy as <span class="math inline">\(H(Y)\)</span>, and the entropy of tea preferences conditioned on sitting in the garden as as <span class="math inline">\(H(Y \vert X = \text{garden})\)</span>. The average value of the conditioned entropy across all rooms is important enough to have a special name: the <em>conditional entropy</em>. We can write it as:</p>
<p><span class="math display">\[H(Y\vert X) \triangleq \sum_{x \in \{\text{garden, left, right}\}} H(Y|X = x) \mathbb{P}(X = x)\;.\]</span></p>
<p>To calculate the conditional entropy, we compute the local entropy conditioned on each room, and then take a weighted average in which each room counts according to its “population.” This weighting is reflected in the factor <span class="math inline">\(\mathbb{P}(X = x)\)</span>, which can be read as “the probability that a randomly selected patron is sitting in room <span class="math inline">\(x\)</span>.” In our example, this is the same as the proportion of patrons in room <span class="math inline">\(x\)</span>. Applying the formula above to the local entropies we calculated earlier, we have that <span class="math inline">\(H(Y\vert X) \approx 1.09\)</span> on Saturday, and is zero on Friday and Sunday.</p>
</div>
<div id="mutual-information-measures-segregation" class="section level1">
<h1>Mutual Information Measures Segregation</h1>
<p>Let’s summarise:</p>
<table>
<thead>
<tr class="header">
<th align="left">Day</th>
<th align="right">Diverse</th>
<th align="right">Segregated</th>
<th align="right"><span class="math inline">\(H(Y)\)</span></th>
<th align="right"><span class="math inline">\(H(Y \vert X)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Friday</td>
<td align="right">No</td>
<td align="right">No</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="left">Saturday</td>
<td align="right">Yes</td>
<td align="right">No</td>
<td align="right">1.10</td>
<td align="right">1.09</td>
</tr>
<tr class="odd">
<td align="left">Sunday</td>
<td align="right">Yes</td>
<td align="right">Yes</td>
<td align="right">1.10</td>
<td align="right">0.00</td>
</tr>
</tbody>
</table>
<p>A pattern is emerging: if diversity is measured by <em>high</em> <span class="math inline">\(H(Y)\)</span>, segregation is measured by <em>high</em> <span class="math inline">\(H(Y)\)</span> and <em>low</em> <span class="math inline">\(H(Y \vert X)\)</span>. The difference between <span class="math inline">\(H(Y)\)</span> and <span class="math inline">\(H(Y \vert X)\)</span> turns out to be a fundamental concept in information theory: the <strong>mutual information</strong>, denoted <span class="math inline">\(I(X,Y)\)</span>.</p>
<p><span class="math display">\[ I(X,Y) \triangleq H(Y) - H(Y \vert X) \;.\]</span></p>
<p>Recall from last post that <span class="math inline">\(H(Y)\)</span> measures the difficulty of a guessing game in which I try to match the preferences of the cafe customers. From our discussion above, we can think of <span class="math inline">\(H(Y\vert X)\)</span> in similar terms: its the difficulty of a guessing game in which I try to match the preferences of customers in <em>in a single room.</em> So, <span class="math inline">\(I(X,Y)\)</span> can be interpreted as the (average) <strong>value of knowing which room I need to serve.</strong> The value is high on Sunday: matching the preferences of the whole shop is hard (high entropy), but if you tell me which room I am serving, the problem is suddenly easy – I only need to accomodate one kind of customer. This phenomenon is measured by a high value of <span class="math inline">\(I(X,Y) = 1.10\)</span>. On Saturday, however, matching the preferences of any individual room is about as hard as matching the preferences of the whole shop: both the entropy <span class="math inline">\(H(Y)\)</span> and conditional entropy <span class="math inline">\(H(Y\vert X)\)</span> are high, giving a low value of <span class="math inline">\(I(X,Y)\)</span>.</p>
<p>Another natural way to say this is that knowing what room I am serving is highly <em>informative</em> on Sunday, but not on Saturday. The mutual information measures exactly how informative that knowledge really is.</p>
<p>Let’s now cash out the role of the mutual information in the measurement of segregation. It’s possible to prove that the only case in which <span class="math inline">\(I(X,Y) = 0\)</span> is perecisely the one in which each room has the same distribution of tea preferences as the shop generally. This obtains on Friday and Saturday, but not Sunday. Furthermore, <span class="math inline">\(I(X,Y)\)</span> achieves its maximum value of <span class="math inline">\(H(Y)\)</span> precisely when each room has just one kind of tea drinker: the Saturday scenario. The larger <span class="math inline">\(I(X,Y)\)</span>, the greater role that your tea preference plays in determining what room you sit in.</p>
<p>Indeed, <span class="math inline">\(I(X,Y)\)</span> turns out to be such a natural measure of segregation that it was reinvented in the sociological community some 25 years after Shannon’s seminal work founding information theory. The frequently used “Information Theory Index” of Theil and Finezza (1971)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> is nothing more than <span class="math inline">\(I(X,Y) / H(Y)\)</span>; the normalization by <span class="math inline">\(H(Y)\)</span> results in a measure that takes values in <span class="math inline">\([0,1]\)</span>. Formally, what <span class="math inline">\(I(X,Y)\)</span> measures is <em>unevenness</em>: the tendency of groups to be differently-distributed in space. Since Theil and Finezza, a number<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> of methodologists<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> have formulated<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> a variety<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> of measures,<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> all of which can be viewed as normalized mutual informations.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<div id="mutual-information-measures-dependence" class="section level2">
<h2>Mutual Information Measures Dependence</h2>
<p>Readers with statistical background might read the above as a statement about dependence: <span class="math inline">\(I(X,Y)\)</span> measures the degree of statistical association between room and tea preference. Let’s step back from the context of diversity for a moment to explore this point. To do this, let’s compare <span class="math inline">\(I(X,Y)\)</span> to a more familiar quantity that does something similar: the correlation coefficient <span class="math inline">\(R^2\)</span>. If you took a statistics class, you might remember linear regression, the problem of drawing a “best fit” line through a cloud of data plotted on the <span class="math inline">\(x-y\)</span> plane. One thing you might have done was to compute “<span class="math inline">\(R^2\)</span>,” which is more formally called the Pearson correlation coefficient. When <span class="math inline">\(R^2 = 1\)</span>, the explanatory variable <span class="math inline">\(x\)</span> is perfectly correlated with the explanandum <span class="math inline">\(y\)</span>, but when <span class="math inline">\(R^2 = 0\)</span>, there’s no linear relationship. One thing that might have been emphasized to you is:</p>
<blockquote>
<p><span class="math inline">\(R^2 = 0\)</span> does not imply that the variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent.</p>
</blockquote>
<p>Here’s a <a href="https://commons.wikimedia.org/wiki/File:Correlation_examples2.svg#/media/File:Correlation_examples2.svg">set of examples</a> that you can view on Wikipedia: the bottom row data sets are all uncorrelated (<span class="math inline">\(R^2 = 0\)</span>), but clearly not independent.</p>
<p>The mutual information <span class="math inline">\(I(X,Y)\)</span> is something like a correlation coefficient on steroids, in the sense that:</p>
<blockquote>
<p><span class="math inline">\(I(X,Y) = 0\)</span> <strong>does</strong> imply that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
</blockquote>
<p>One nice thing about this result is that it actually makes precise an intuition about probabilistic dependence that many of us share: two variables are dependent if knowing one “gives you information” about the other. <span class="math inline">\(I(X,Y)\)</span> quantifies this idea and makes it rigorously correct.</p>
</div>
</div>
<div id="the-fundamental-equation-of-segregation-and-diversity" class="section level1">
<h1>The Fundamental Equation of Segregation and Diversity</h1>
<p>Returning to segregation and diversity, we’re ready to write out a lovely and simple equation relating diversity and segregation across spatial scales. Let’s go back to the definition of <span class="math inline">\(I(X,Y)\)</span> above. Let’s recall first that <span class="math inline">\(H(Y)\)</span> measures the diversity of the entire shop. Next, the conditional entropy <span class="math inline">\(H(Y|X)\)</span> is a measure of average diversity in each room. In the context of segregation studies, <span class="math inline">\(H(Y|X)\)</span> measures <strong>exposure</strong> – the diversity that an individual experiences in their local environment. From my individual perspective, the diversity of the entire shop doesn’t matter – just the diversity of my immediate surroundings. Finally, as we discussed above, the mutual information <span class="math inline">\(I(X,Y)\)</span> measures the segregation of the entire shop. If we take the definition of <span class="math inline">\(I(X,Y) = H(Y) - H(Y|X)\)</span> and rearrange the terms, we wind up with what I think of as the fundamental equation of segregation and diversity:</p>
<p><span class="math display">\[\text{Global Diversity} = \text{Local Exposure} + \text{Global Segregation}\]</span></p>
<p>What’s striking about this equation is not surprise or complexity, but the way it so neatly organizes natural concepts into a quantitative order. The intuition here is simple: if there are many groups coexisting in a system (global diversity is high), then they must exist either in the same spaces (local exposure is high) or in difference spaces (global segregation is high). One of my favorite points about information theory is its ability to give precise quantitative formulations for common sense ideas about structure and learning; this equation is a prime example.</p>
</div>
<div id="whats-next" class="section level1">
<h1>What’s Next</h1>
<p>Thanks to everyone who checked in for this two-part series! We covered the elements of an information-theoretic view of diversity and segregation. My aim in future posts is to discuss how to do practical computation with these measures. I’ll also discuss some of my research into how to analyze diversity and segregation at multiple spatial scales, and how to algorithmically find spatial structure in social systems.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Theil, H., &amp; Finezza, A. J. (1971). A note on the measurement of racial integration of schools by means of informational concepts. Taylor and Francis.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Reardon, S. F. (2008). Measures of Ordinal Segregation. In Y. Flückiger, S. F. Reardon, &amp; J. Silber (Eds.), Occupational and Residential Segregation (Research on Economic Inequality, Volume 17) (pp. 129–155).<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Reardon, S. F., &amp; O’Sullivan, D. (2004). Measures of Spatial Segregation. Sociological Methodology, 34(1), 121–162. <a href="http://doi.org/10.1111/j.0081-1750.2004.00150.x" class="uri">http://doi.org/10.1111/j.0081-1750.2004.00150.x</a><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Reardon, S. F., &amp; Firebaugh, G. (2002). Measures of multigroup segregation. Sociological Methodology, 32, 33–67. <a href="http://doi.org/10.1111/1467-9531.00110" class="uri">http://doi.org/10.1111/1467-9531.00110</a><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Jargowsky, P. A., &amp; Kim, J. (2005). A measure of spatial segregation: The generalized neighborhood sorting index. National Poverty Center Working Paper Series. Retrieved from <a href="http://nationalpovertycenter.net/publications/workingpaper05/paper03/jargowsky_kim_21mar2005.pdf" class="uri">http://nationalpovertycenter.net/publications/workingpaper05/paper03/jargowsky_kim_21mar2005.pdf</a><a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Roberto, E. (2015). Measuring Inequality and Segregation. arXiv.org, 1–26. Retrieved from <a href="http://arxiv.org/abs/1508.01167" class="uri">http://arxiv.org/abs/1508.01167</a><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Roberto, E. (2016). The Spatial Context of Residential Segregation. arXiv.org, 1–27.<a href="#fnref7">↩</a></p></li>
</ol>
</div>
